{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Time2Vector(nn.Module):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.lin = nn.Linear(seq_len, 1)\n",
    "        self.per = nn.Linear(seq_len, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_lin = self.lin(x) \n",
    "        x_lin = x.unsqueeze() # (batch, seq_len, 1)\n",
    "\n",
    "        x_per = torch.sin(self.per(x))\n",
    "        x_lin = x.unsqueeze() # (batch, seq_len, 1)\n",
    "        return torch.concat([time_linear, time_periodic], axis=-1) # (batch, seq_len, 2)\n",
    "    \n",
    "    \n",
    "\n",
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_v, seq_len):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.Q = nn.Linear(seq_len, d_k)\n",
    "        self.K = nn.Linear(seq_len, d_k)\n",
    "        self.V = nn.Linear(seq_len, d_v)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        \n",
    "        attn_weights = torch.matmul(q, k.T)\n",
    "        attn_weights = attn_weights / torch.sqrt(self.d_k)\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "        \n",
    "        attn_out = torch.matmul(attn_weights, v)\n",
    "        return attn_out  \n",
    "    \n",
    "    \n",
    "class MultiAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_v, n_heads, seq_len):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.seq_len = seq_len\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "        \n",
    "        for n in range(self.n_heads):\n",
    "              self.attn_heads.append(SingleAttention(self.d_k, self.d_v, seq_len))  \n",
    "        self.linear = nn.Linear(input_shape, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn = [self.attn_heads[i](x) for i in range(self.n_heads)]\n",
    "        concat_attn = torch.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear \n",
    "    \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, seq_len, dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads, seq_len)\n",
    "        self.attn_dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = nn.LayerNorm(input_shape, eps=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = nn.Conv1D(1, self.ff_dim, kernel_size=1, activation='relu')\n",
    "        self.ff_conv1D_2 = nn.Conv1D(self.ff_dim, filters=3, kernel_size=1) # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
    "        self.ff_dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = nn.LayerNorm(input_shape, eps=1e-6)    \n",
    "\n",
    " \n",
    "  \n",
    "    def call(self, x): # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(x)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(x[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(x[0] + ff_layer)\n",
    "        return ff_layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, seq_len, dropout=0.1, **kwargs):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.time_embedding = Time2Vector(seq_len)\n",
    "        self.attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim, seq_len)\n",
    "        self.attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim, seq_len)\n",
    "        self.attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim, seq_len)\n",
    "        self.pooling = nn.AvgPool1d()\n",
    "        \n",
    "        self.dropout(dropout)\n",
    "        self.lin1 = nn.Linear(3, 64)\n",
    "        self.lin2 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        in_seq = time_embedding(x.shape[1])\n",
    "        x = torch.concat([in_seq, x], axis=-1)\n",
    "        x = attn_layer1((x, x, x))\n",
    "        x = attn_layer2((x, x, x))\n",
    "        x = attn_layer3((x, x, x))\n",
    "        \n",
    "        x = self.dropout(self.pooling(x))\n",
    "        x = self.dropout(self.relu(self.lin1(x)))\n",
    "        \n",
    "        out = self.lin2(x)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "        \n",
    "def create_model():\n",
    "    '''Initialize time and transformer layers'''\n",
    "    time_embedding = Time2Vector(seq_len)\n",
    "    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "\n",
    "    '''Construct model'''\n",
    "    in_seq = Input(shape=(seq_len, 5))\n",
    "    x = time_embedding(in_seq)\n",
    "    x = Concatenate(axis=-1)([in_seq, x])\n",
    "    x = attn_layer1((x, x, x))\n",
    "    x = attn_layer2((x, x, x))\n",
    "    x = attn_layer3((x, x, x))\n",
    "    x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=in_seq, outputs=out)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
