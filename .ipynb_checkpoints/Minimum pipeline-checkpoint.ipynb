{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.manifold import TSNE, Isomap\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import (KMeans, SpectralClustering, \n",
    "                             MiniBatchKMeans, AgglomerativeClustering)\n",
    "\n",
    "from sklearn.metrics import (davies_bouldin_score, \n",
    "                            silhouette_score,\n",
    "                            calinski_harabasz_score,\n",
    "                            homogeneity_score)\n",
    "\n",
    "from utils.portfolio import MarkowitzPortfolio, backtesting_universal\n",
    "from utils.portfolio_metrics import (calculate_measures, show_drawdown_recovery, \n",
    "                                     find_max_recovery, find_max_drawdown)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "\n",
    "from utils.portfolio_metrics import calculate_measures, show_drawdown_recovery\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "    \n",
    "rs = config['random_state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config['ticker_data_preprocessed'], index_col=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct = df.drop(['sector'], axis=1).T\n",
    "df_pct.index = pd.to_datetime(df_pct.index)\n",
    "\n",
    "tickers_list = df_pct.columns.tolist()\n",
    "\n",
    "df_pct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market = pd.read_csv(config['ticker_data_sp500'], index_col=0)\n",
    "df_market.index = pd.to_datetime(df_market.index)\n",
    "df_market = df_market.pct_change()\n",
    "df_market.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors = pd.read_csv(config['tickers_sectors_path'], index_col=0)\n",
    "df_sectors.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats_tsne = TSNE(n_components=2).fit_transform(df.drop(['sector'], axis=1))\n",
    "# df_tsne = pd.DataFrame({'axis0':feats_tsne[:, 0],'axis1':feats_tsne[:, 1],'sector':df['sector']})\n",
    "\n",
    "# fig = px.scatter(df_tsne, x = 'axis0', y = 'axis1', color=\"sector\", width=800, height=600)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(parameters):\n",
    "    if not parameters:\n",
    "        yield dict()\n",
    "    else:\n",
    "        key_to_iterate = list(parameters.keys())[0]\n",
    "        next_round_parameters = {p : parameters[p]\n",
    "                    for p in parameters if p != key_to_iterate}\n",
    "        for val in parameters[key_to_iterate]:\n",
    "            for pars in make_generator(next_round_parameters):\n",
    "                temp_res = pars\n",
    "                temp_res[key_to_iterate] = val\n",
    "                yield temp_res\n",
    "\n",
    "\n",
    "class ClusteringGridSearch:\n",
    "    def __init__(self, estimator, param_grid, scoring):\n",
    "\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.scoring = scoring\n",
    "        \n",
    "        self.best_params_=dict()\n",
    "        self.best_estimator_ = None\n",
    "        self.best_score_ = - 1e-8\n",
    "        \n",
    "    def fit(self, X):\n",
    "        all_params = self.estimator.get_params()\n",
    "        \n",
    "        for params in make_generator(self.param_grid):\n",
    "            \n",
    "            all_params.update(params)\n",
    "            self.estimator = self.estimator.set_params(**all_params)\n",
    "            score = self.scoring(self.estimator, X)\n",
    "    \n",
    "            if score > self.best_score_:\n",
    "                self.best_score_ = score\n",
    "                self.best_estimator_ = self.estimator.fit(X)\n",
    "                self.best_params_ = params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(models_dict, df_pct):\n",
    "    embeddings_dict = dict()\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        embeddings_dict[model_name] = model.transform(df_pct)\n",
    "        \n",
    "    return embeddings_dict\n",
    "\n",
    "def get_clusters(data,\n",
    "                 tickers_list,\n",
    "                 clust_model,\n",
    "                 make_grid=False, \n",
    "                 grid_params=None,\n",
    "                 grid_metric=None):\n",
    "    \n",
    "    \n",
    "    if make_grid:\n",
    "        grid_model = ClusteringGridSearch(estimator=clust_model, param_grid=grid_params,\n",
    "                                            scoring=grid_metric)\n",
    "        grid_model.fit(data)\n",
    "        clust_model = grid_model.best_estimator_\n",
    "    else:\n",
    "        clust_model.fit(data)\n",
    "    df_clusters = pd.DataFrame([tickers_list, clust_model.labels_], index=['ticker', 'cluster']).T\n",
    "    return df_clusters\n",
    "\n",
    "\n",
    "def select_assets(df_clusters, df_pct, selection_method, n_save=2, **kargs):\n",
    "\n",
    "    selected_tickers = []\n",
    "    for cluster in np.unique(df_clusters['cluster'].values):\n",
    "\n",
    "        df_clusters_loc = df_clusters[df_clusters['cluster'] == cluster]\n",
    "        list_tickers = df_clusters_loc['ticker'].values.tolist()\n",
    "        selected_tickers_loc = selection_method(list_tickers, n_save=n_save, df_pct=df_pct, **kargs)\n",
    "        selected_tickers.extend(selected_tickers_loc)\n",
    "        \n",
    "    return selected_tickers\n",
    "\n",
    "def get_train_test_data(df_pct,\n",
    "                        train_start_per, \n",
    "                        window_train, \n",
    "                        window_test):\n",
    "    \n",
    "    # slicing data train\n",
    "            \n",
    "    train_finish_per = train_start_per + window_train\n",
    "\n",
    "    train_year_start_per = train_start_per // 12\n",
    "    train_month_start_per = train_start_per % 12 + 1\n",
    "\n",
    "    train_year_finish_per = train_finish_per // 12\n",
    "    train_month_finish_per = train_finish_per % 12 + 1\n",
    "\n",
    "    mask_train = (df_pct.index > datetime(train_year_start_per, train_month_start_per, 1))\n",
    "    mask_train = mask_train & (df_pct.index < datetime(train_year_finish_per, train_month_finish_per, 1))\n",
    "    df_train = df_pct[mask_train]\n",
    "\n",
    "    # slicing data test\n",
    "    \n",
    "    test_finish_per = train_finish_per + window_test\n",
    "    \n",
    "    test_year_start_per = train_year_finish_per\n",
    "    test_month_start_per = train_month_finish_per\n",
    "\n",
    "    test_year_finish_per = test_finish_per // 12\n",
    "    test_month_finish_per = test_finish_per % 12 + 1\n",
    "\n",
    "    #print(train_year_start_per, train_month_start_per,  train_year_finish_per, train_month_finish_per, test_year_finish_per, test_month_finish_per)\n",
    "\n",
    "    mask_test = (df_pct.index > datetime(test_year_start_per, test_month_start_per, 1)) \n",
    "    mask_test = mask_test & (df_pct.index < datetime(test_year_finish_per, test_month_finish_per, 1))\n",
    "    df_test = df_pct[mask_test]\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def backtesting_one_model(df_pct, # df with pct_changes: columns - tick, index - date\n",
    "                        port_model=MarkowitzPortfolio, #portfolio estimation function\n",
    "                        window_train=24, # size of train window in months\n",
    "                        window_test=1,  # size of train window in months\n",
    "                        train_start_year=2018, #start data year\n",
    "                        train_start_month=1, #start data month \n",
    "                        test_finish_year=2022, #end data year\n",
    "                        test_finish_month=11, #end data month\n",
    "                        **kargs):\n",
    "    \n",
    "    weights_all = []\n",
    "    return_portfolio = pd.DataFrame([])\n",
    "    \n",
    "    train_start_month = train_start_year * 12 + train_start_month - 1 #indexing from 0\n",
    "    test_finish_month = test_finish_year * 12 + test_finish_month - 1 #indexing from 0\n",
    "    train_finish_month = test_finish_month - window_train - window_test + 1\n",
    "    \n",
    "    for train_start_per in range(train_start_month, train_finish_month, window_test):\n",
    "        \n",
    "        df_train, df_test = get_train_test_data(df_pct, train_start_per, window_train, window_test)\n",
    "        \n",
    "        mu = (((df_train + 1).prod()) ** (1 / len(df_train)) - 1).values * 252  # средняя доходность за год (252 раб дня)\n",
    "        Sigma = df_train.cov().values * 252  # ковариационная матрица за год (252 раб дня)\n",
    "\n",
    "        port_ = port_model(mu, Sigma, kargs=kargs)\n",
    "        weights, _ = port_.fit()\n",
    "        \n",
    "        weights_all.append(weights)\n",
    "        \n",
    "        return_portfolio_loc = pd.DataFrame(df_test.values @ weights, index=df_test.index)\n",
    "        return_portfolio = pd.concat([return_portfolio, return_portfolio_loc])\n",
    "            \n",
    "    return weights_all, return_portfolio\n",
    "\n",
    "def clustering_estimation(X, labels):\n",
    "    scores = []\n",
    "    scores.append(davies_bouldin_score(X, labels)) # Davies-Bouldin Index\n",
    "    scores.append(calinski_harabasz_score(X, labels)) # Calinski Harabaz Index\n",
    "    scores.append(silhouette_score(X, labels)) # Silhouette Coefficient\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_pipeline(df_pct,\n",
    "                     df_market,\n",
    "                     embedding_data,\n",
    "                     \n",
    "                     clust_params,\n",
    "                     selection_params,\n",
    "                     backtesting_params,\n",
    "                    ):\n",
    "    \n",
    "    tickers_list = df_pct.columns.tolist()\n",
    "    \n",
    "    #make clustering\n",
    "    df_clusters = get_clusters(embedding_data, tickers_list, **clust_params)\n",
    "    print(df_clusters.info())\n",
    "    clust_metrics = clustering_estimation(embedding_data, df_clusters['cluster'])\n",
    "    \n",
    "    #stock selection\n",
    "    selected_tickers = select_assets(df_clusters, df_pct, **selection_params)\n",
    "    \n",
    "    df_pct_loc = df_pct.copy()\n",
    "    df_pct_loc = df_pct_loc[selected_tickers]\n",
    "    \n",
    "    #port_modelling\n",
    "    weights_all, return_portfolio = backtesting_one_model(df_pct_loc, # df with pct_changes: columns - tick, index - date\n",
    "                        **backtesting_params)\n",
    "    \n",
    "    return weights_all, return_portfolio, clust_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(estimator, X, y=None):\n",
    "    estimator.fit(X)\n",
    "    labels_predicted = estimator.labels_\n",
    "    score = silhouette_score(X, labels_predicted)\n",
    "    return score\n",
    "\n",
    "def selection_sharp(list_tickers, n_save, df_pct, riskfree_rate):\n",
    "    df_pct = df_pct[list_tickers]\n",
    "    \n",
    "    sharp = (df_pct.mean() - riskfree_rate)/df_pct.std()\n",
    "    selected_tickers = sharp.sort_values(ascending=False).head(n_save).index.tolist()\n",
    "    \n",
    "    return selected_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict =  {'pca': PCA(n_components=100, random_state=rs).fit_transform(df.drop(['sector'], axis=1)),\n",
    "            'fast_ica': FastICA(n_components=100, random_state=rs).fit_transform(df.drop(['sector'], axis=1))\n",
    "            }\n",
    "\n",
    "tickers = df.index.tolist()\n",
    "\n",
    "df_conv = pd.read_csv(config['nn_conv_data'], index_col=0).loc[tickers]\n",
    "df_mlp = pd.read_csv(config['nn_mlp_data'], index_col=0).loc[tickers]\n",
    "df_lstm = pd.read_csv(config['nn_lstm_data'], index_col=0).loc[tickers]\n",
    "\n",
    "emb_dict['neural_conv'] = df_conv.values\n",
    "emb_dict['neural_mlp'] = df_mlp.values\n",
    "emb_dict['neural_lstm'] = df_lstm.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_tsfresh = df_pct.reset_index()\n",
    "df_to_tsfresh = pd.melt(df_to_tsfresh, id_vars=['index'], var_name='ticker')\n",
    "df_to_tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tsfresh = extract_features(df_to_tsfresh, column_id='ticker', column_sort='index')\n",
    "\n",
    "features_filtered = SelectKBest(k=100).fit_transform(data_tsfresh.dropna(axis=1), df['sector'])\n",
    "emb_dict['tsfresh'] = features_filtered  # n_instances x output_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ts2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts2vec.ts2vec import TS2Vec\n",
    "\n",
    "data = np.expand_dims(df_pct.values.T, axis=2)\n",
    "\n",
    "# Train a TS2Vec model\n",
    "model = TS2Vec(\n",
    "    input_dims=1,\n",
    "    device=0,\n",
    "    output_dims=100\n",
    ")\n",
    "loss_log = model.fit(\n",
    "    data,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "emb_dict['ts2vec'] = model.encode(data, encoding_window='full_series')  # n_instances x output_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_features = pd.read_csv(config['features_path'], index_col=0)\n",
    "table_features = StandardScaler().fit_transform(table_features)\n",
    "\n",
    "emb_dict['table_data'] = table_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_name, data in emb_dict.items():\n",
    "    print(method_name, np.sum(np.isnan(data)), data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riskfree = config['riskless_rate']\n",
    "riskfree_rate=(1 + riskfree) **(1/252) - 1,\n",
    "ret_det=(1 + 0.03) **(1/252) - 1,\n",
    "    \n",
    "    \n",
    "clust_params = {'clust_model':KMeans(n_clusters=11, random_state=42),\n",
    "                'make_grid':False, \n",
    "                'grid_params':{\n",
    "                   'n_clusters':np.arange(9, 14),\n",
    "                   'init': ['k-means++', 'random'],\n",
    "                   'algorithm':['auto', 'full', 'elkan']\n",
    "                },\n",
    "                'grid_metric':custom_score}\n",
    "\n",
    "\n",
    "selection_params = {'selection_method':selection_sharp,\n",
    "                    'n_save':2, \n",
    "                    'riskfree_rate':riskfree_rate,}\n",
    "\n",
    "backtesting_params = {'port_model':MarkowitzPortfolio,\n",
    "                      'window_train':24, # size of train window in months\n",
    "                      'window_test':1,  # size of train window in months\n",
    "                       \n",
    "                      'train_start_year':2018, #start data year\n",
    "                      'train_start_month':1, #start data month \n",
    "                      'test_finish_year':2022, #end data year\n",
    "                      'test_finish_month':11, #end data month\n",
    "                      'ret_det':ret_det\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metrics_df = pd.DataFrame() \n",
    "port_df = pd.DataFrame()    #Считаем портфель Марковица для всех методов кластеризации\n",
    "dict_weight_methods = dict()\n",
    "\n",
    "for model_name, embedding_data in tqdm(emb_dict.items()):\n",
    "    \n",
    "    weights_all, return_portfolio, cluster_metrics = general_pipeline(\n",
    "        df_pct,\n",
    "        df_market,\n",
    "        embedding_data=embedding_data,\n",
    "        clust_params=clust_params,\n",
    "        selection_params=selection_params,\n",
    "        backtesting_params=backtesting_params)\n",
    "    \n",
    "    cluster_metrics_df[model_name] = cluster_metrics\n",
    "    port_df[model_name] = return_portfolio\n",
    "    dict_weight_methods[model_name] = weights_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tslearn clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans, KernelKMeans, KShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tslearn_models = { 'TimeSeriesKMeans' : TimeSeriesKMeans(n_clusters=11, metric=\"dtw\", max_iter=5, random_state=0),\n",
    "                  'KernelKMeans' : KernelKMeans(n_clusters=11, kernel=\"gak\", random_state=0),\n",
    "                  'KShape' : KShape(n_clusters=11, n_init=1, random_state=0)\n",
    "                 }\n",
    "\n",
    "\n",
    "tslearn_params = {\n",
    "    'TimeSeriesKMeans' : {\n",
    "                   'n_clusters':np.arange(9, 14),\n",
    "                   'init': ['k-means++', 'random'],\n",
    "                   'algorithm':['auto', 'full', 'elkan']\n",
    "                },\n",
    "    'KernelKMeans' : {\n",
    "                   'n_clusters':np.arange(9, 14),\n",
    "                   'init': ['k-means++', 'random'],\n",
    "                   'algorithm':['auto', 'full', 'elkan']\n",
    "                },\n",
    "    'KShape' : {\n",
    "                   'n_clusters':np.arange(9, 14),\n",
    "                   'init': ['k-means++', 'random'],\n",
    "                   'algorithm':['auto', 'full', 'elkan']\n",
    "                }\n",
    "    \n",
    "}\n",
    "\n",
    "clust_params = {#'clust_model':KMeans(n_clusters=11, random_state=42),\n",
    "                'make_grid':False, \n",
    "#                 'grid_params':{\n",
    "#                    'n_clusters':np.arange(9, 14),\n",
    "#                    'init': ['k-means++', 'random'],\n",
    "#                    'algorithm':['auto', 'full', 'elkan']\n",
    "#                 },\n",
    "                'grid_metric':custom_score}\n",
    "\n",
    "for model_name, model in tqdm(tslearn_models.items()):\n",
    "    clust_params['clust_model'] = model\n",
    "    clust_params['grid_params'] = tslearn_params[model_name]\n",
    "    \n",
    "    weights_all, return_portfolio, cluster_metrics = general_pipeline(\n",
    "        df_pct,\n",
    "        df_market,\n",
    "        embedding_data=df.drop(['sector'], axis=1).values,\n",
    "        clust_params=clust_params,\n",
    "        selection_params=selection_params,\n",
    "        backtesting_params=backtesting_params)\n",
    "    \n",
    "    cluster_metrics_df['tsclust_'+model_name] = cluster_metrics\n",
    "    port_df['tsclust_'+model_name] = return_portfolio\n",
    "    dict_weight_methods['tsclust_'+model_name] = weights_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_df['sp500'] = df_market.loc[port_df.index] \n",
    "\n",
    "# economic sectors\n",
    "clust_econ_sectors = LabelEncoder().fit_transform(df['sector'])\n",
    "df_clusters = pd.DataFrame([tickers_list, clust_econ_sectors], index=['ticker', 'cluster']).T\n",
    "\n",
    "\n",
    "selected_tickers = select_assets(df_clusters, df_pct, **selection_params)\n",
    "df_pct_loc = df_pct[selected_tickers]\n",
    "weights_all, return_portfolio = backtesting_one_model(df_pct_loc, # df with pct_changes: columns - tick, index - date\n",
    "                    **backtesting_params)\n",
    "\n",
    "cluster_metrics_df['sectors'] = clustering_estimation(df_pct.T.values, df_clusters['cluster'])\n",
    "dict_weight_methods['sectors'] = weights_all\n",
    "port_df['sectors'] = return_portfolio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(port_df, df_market, riskfree_rate, port_name='Markovitz'):\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    #Average daily returns\n",
    "    mean = port_df.mean()\n",
    "    result_df['AVG_returns'] = mean \n",
    "\n",
    "    #Risk\n",
    "    risk = port_df.std()\n",
    "    result_df['Risk'] = risk\n",
    "\n",
    "    #Beta\n",
    "    var_ = port_df.var()\n",
    "    cov_ = port_df.cov()\n",
    "    beta = cov_['sp500']/var_\n",
    "\n",
    "    result_df['Beta'] = beta\n",
    "\n",
    "    #Alpha\n",
    "    alpha = mean - (riskfree_rate + beta*(result_df.loc['sp500', 'AVG_returns'] - riskfree_rate))\n",
    "    result_df['Alpha'] = alpha\n",
    "    \n",
    "    #Sharpe \n",
    "    sharpe = (mean - riskfree_rate)/risk\n",
    "    result_df['Sharpe'] = sharpe\n",
    "\n",
    "    #VaR(95%)\n",
    "    VaR = - risk*1.65\n",
    "    result_df['VaR(95%)'] = VaR\n",
    "    \n",
    "    #Drawdown and Recovery\n",
    "    portfolio_value = (port_df+1).cumprod() #датафрейм со \"стоимостью\" портфеля\n",
    "\n",
    "    recovery = []\n",
    "    drawdown = []\n",
    "    for i in range(len(port_df.columns)):\n",
    "        recovery.append(find_max_recovery(portfolio_value.iloc[:,i])[0])\n",
    "        drawdown.append(find_max_drawdown(portfolio_value.iloc[:,i])[0])\n",
    "    \n",
    "    result_df['Drawdown(%)'] = drawdown\n",
    "    result_df['Recovery(days)'] = recovery\n",
    "     \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = calc_metrics(port_df, df_market, riskfree_rate)\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metrics_df = cluster_metrics_df.rename(index=dict(zip(list(range(3)), ['DB', 'HC', 'Sil']))).T\n",
    "cluster_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sil -> greater better (from -1 to 1)\n",
    "\n",
    "DB -> less better\n",
    "\n",
    "HC -> greater better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
